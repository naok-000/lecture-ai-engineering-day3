{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/naok-000/lecture-ai-engineering-day3/blob/main/ai_engineering_03_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AIエンジニアリング実践講座 第3回課題\n",
        "\n",
        "- 講義で学んだRAG(Retrieval-Augmented Generation)技術を用いて、LLMの生成内容を改善する実践的な取り組みを行った．演習で利用したコードをベースに、独自の質問と参照文書を用いて実験を行い、RAGの効果を定量的・定性的に評価した．\n",
        "- プログラムはGoogle Colab（無料版）T4 GPU を利用して実行した．"
      ],
      "metadata": {
        "id": "u1MJIa3GhhOu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM50WAI7GXwC",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install google-colab-selenium\n",
        "!pip install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V2PStE0uqM03"
      },
      "outputs": [],
      "source": [
        "# 演習用のコンテンツを取得\n",
        "!git clone https://github.com/naok-000/lecture-ai-engineering-day3.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXo_kFASXlvp"
      },
      "outputs": [],
      "source": [
        "# HuggingFace Login\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ_NUIftXwLc"
      },
      "outputs": [],
      "source": [
        "# CUDAが利用可能ならGPUを、それ以外ならCPUをデバイスとして設定\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7eTgV8XBPA90"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tV9mO8oXoaM"
      },
      "outputs": [],
      "source": [
        "# モデル(Gemma2)の読み込み\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"google/gemma-2-2b-jpn-it\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=False,\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            device_map=\"auto\",\n",
        "            quantization_config=bnb_config,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piTdVxTfGcc_"
      },
      "source": [
        "## 1. ベースラインモデル評価\n",
        "\n",
        "まずはベースモデルがどの程度知識を持っているか確かめる"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_output(query):\n",
        "  messages = [\n",
        "      {\"role\": \"user\", \"content\": query},\n",
        "  ]\n",
        "  input_ids = tokenizer.apply_chat_template(\n",
        "      messages,\n",
        "      add_generation_prompt=True,\n",
        "      return_tensors=\"pt\"\n",
        "  ).to(model.device)\n",
        "\n",
        "  terminators = [\n",
        "      tokenizer.eos_token_id,\n",
        "      tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
        "  ]\n",
        "\n",
        "  outputs = model.generate(\n",
        "      input_ids,\n",
        "      max_new_tokens=1024,\n",
        "      eos_token_id=terminators,\n",
        "      do_sample=False,\n",
        "      # temperature=0.6, # If do_sample=True\n",
        "      # top_p=0.9,  # If do_sample=True\n",
        "  )\n",
        "\n",
        "  response = outputs[0][input_ids.shape[-1]:]\n",
        "  return tokenizer.decode(response, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "huDNYbXW3lOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBUZ3o6dhMlC"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"what are the main vulnerabilities of the OBD-II port that make it a critical entry point for vehicle malware attacks?\",\n",
        "    \"OBD-IIポートが車両のマルウェア攻撃における重要な侵入口となる主な脆弱性は何ですか？\",\n",
        "    \"How do different wireless technologies (such as Bluetooth, Wi-Fi, Cellular, and DSRC) collectively contribute to the increased malware attack surface in modern intelligent vehicles?\",\n",
        "    \"Bluetooth、Wi-Fi、セルラー、DSRC などの無線通信技術は、現代のインテリジェント車両におけるマルウェア攻撃の対象領域をどのようにして拡大しているのですか？\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    print(f\"question: {question}\")\n",
        "    response = generate_output(question)\n",
        "    print(f\"response: \\n{response}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "questions = [\n",
        "    \"What is the primary benefit of deploying Fixed Sensor Nodes (FSNs) in Vehicle-to-Infrastructure (V2I) cooperative systems, especially in complex urban scenarios?\",\n",
        "    \"固定センサノード（FSN）をVehicle-to-Infrastructure（V2I）協調システムに導入する主な利点は何ですか？特に複雑な都市環境においては？\",\n",
        "    \"How do cooperative object detection and cooperative tracking complement each other in enhancing safety and reliability in V2I autonomous driving systems, and what specific mechanisms enable this synergy?\",\n",
        "    \"協調型物体検出と協調型追跡は、V2I型自動運転システムにおける安全性と信頼性の向上にどのように相互補完的に働きますか？また、この相乗効果を可能にする具体的なメカニズムは何ですか？\",\n",
        "    \"Who is the author of 'Infrastructure Assisted Autonomous Driving: Research, Challenges, and Opportunities'?\",\n",
        "    \"'Infrastructure Assisted Autonomous Driving: Research, Challenges, and Opportunities'の著者は誰ですか？\"\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    print(f\"question: {question}\")\n",
        "    response = generate_output(question)\n",
        "    print(f\"response: \\n{response}\")"
      ],
      "metadata": {
        "id": "QDH6jHvOJr_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4R-hiKNGyJd"
      },
      "source": [
        "## 2. RAGの活用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47GvcceyObAl"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "emb_model = SentenceTransformer(\"infly/inf-retriever-v1-1.5b\", trust_remote_code=True)\n",
        "# In case you want to reduce the maximum length:\n",
        "emb_model.max_seq_length = 4096"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "資料をpdfからテキストに変換する\n",
        "\n",
        "参考：https://zenn.dev/quiver/articles/21c2978cf869db\n"
      ],
      "metadata": {
        "id": "Ek7Hhuy9QNbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pypdfium2\n",
        "import pypdfium2 as pdfium\n",
        "\n",
        "def pdf_to_text(pdf):\n",
        "    all_text = \"\"\n",
        "    for page in pdf:\n",
        "        textpage = page.get_textpage()\n",
        "        text = textpage.get_text_range()\n",
        "        all_text += text\n",
        "    return all_text\n",
        "\n",
        "pdf = pdfium.PdfDocument('/content/lecture-ai-engineering-day3/data/Infrastructure_Assisted_Autonomous_Driving_Research_Challenges_and_Opportunities.pdf')\n",
        "\n",
        "raw_writedown = pdf_to_text(pdf)\n",
        "\n",
        "documents = [text.strip() for text in raw_writedown.split(\".\")]\n",
        "\n",
        "print(documents)"
      ],
      "metadata": {
        "id": "MM9d22_NP2zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ドキュメントからLLMに渡す参考資料を抽出する"
      ],
      "metadata": {
        "id": "-bX68OAXfkSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ドキュメントから質問に関連するものを抽出し，参考資料を作成\n",
        "def generate_ref(question, documents):\n",
        "  print(f\"question: {question}\\n\")\n",
        "  # Retrievalの実行\n",
        "  query_embeddings = emb_model.encode([question], prompt_name=\"query\")\n",
        "  document_embeddings = emb_model.encode(documents)\n",
        "\n",
        "  # 各ドキュメントの類似度スコア\n",
        "  scores = (query_embeddings @ document_embeddings.T) * 100\n",
        "  print(scores.tolist())\n",
        "\n",
        "  topk = 5\n",
        "  references = []\n",
        "  for ref in [\".\".join(documents[max(0, i-2): min(i+2, len(documents))]).strip() for i in scores.argsort()[0][::-1][:topk]]:\n",
        "\n",
        "    query = f\"与えられた[参考資料]が[質問]に直接関連しているかを、'yes''no'で答えること。[参考資料]\\n{ref}\\n\\n[質問] {question}\"\n",
        "    response = generate_output(query)\n",
        "\n",
        "    print(\"\\n\\n対象となるドキュメント:\\n\", ref.replace(\".\", \".\\n\"))\n",
        "    print(\"\\n関連しているかどうか: \", response)\n",
        "\n",
        "    if \"yes\" in response.lower():\n",
        "      references.append(ref)\n",
        "  return references"
      ],
      "metadata": {
        "id": "UOv72w7qOzqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "references_list = []\n",
        "for question in questions:\n",
        "  references_list.append(generate_ref(question, documents))\n",
        "\n",
        "print (references_list)"
      ],
      "metadata": {
        "id": "vMUi87ElTDBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "参考資料を渡して質問に回答させる"
      ],
      "metadata": {
        "id": "OKLj4X-6frdL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(questions)):\n",
        "  print(f\"question: {questions[i]}\")\n",
        "  query =  f\"[参考資料]\\n{references_list[i]}\\n\\n[質問] {questions[i]}\"\n",
        "  response = generate_output(query)\n",
        "  print(f\"response: \\n{response}\")"
      ],
      "metadata": {
        "id": "dYWREh0FY5e1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}